{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the libraries needed\n",
    "import nltk # Natural Language Processing Library\n",
    "import pickle # For The Model\n",
    "import string # For String Operations\n",
    "import pandas as pd # For DataFrame\n",
    "\n",
    "# Functions from the libraries\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\norbe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\norbe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\norbe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\norbe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting variables\n",
    "stemmer = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "eng_stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(document):\n",
    "\t# Change into lowercase and tokenize\n",
    "\twords = word_tokenize(document.lower())\n",
    "\t\n",
    "\t# Lemmatizing and Stemming\n",
    "\twords = [wnl.lemmatize(word) for word in words]\n",
    "\twords = [stemmer.stem(word) for word in words]\n",
    "\n",
    "\t# Check if words are not in stop_words and only consists of alphabetic\n",
    "\treturn {word: True for word in words if word not in eng_stopwords and word.isalpha()}\n",
    "\n",
    "def trainModel():\n",
    "\tdataset = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "\tfeatures_sets = [(preprocessing(text), label) for text, label in zip(dataset[\"text\"], dataset[\"label\"])]\n",
    "\n",
    "\tshuffle(features_sets)\n",
    "\n",
    "\tsplit_index = int(len(features_sets) * .85)\n",
    "\ttrain_set, test_set = features_sets[:split_index], features_sets[split_index:]\n",
    "\n",
    "\t# Training the model (Naive Bayes)\n",
    "\tclassifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "\t# Testing accuracy\n",
    "\taccuracy = nltk.classify.accuracy(classifier, test_set)\n",
    "\tprint(\"Accuracy: \", accuracy)\n",
    "\n",
    "\t# Print 5 most informative features\n",
    "\tclassifier.show_most_informative_features(5)\n",
    "\n",
    "\t# Save the trained model using pickle\n",
    "\tfile = open(\"model.pickle\", \"wb\")\n",
    "\tpickle.dump(classifier, file)\n",
    "\tfile.close()\n",
    "\n",
    "\treturn classifier\n",
    "\n",
    "def readModel():\n",
    "    # Check the model is available or not\n",
    "    \n",
    "    # If the model is available\n",
    "\ttry:\n",
    "\t\tfile = open(\"model.pickle\", \"rb\") # Read Binary\n",
    "\t\tprint(\"Model is available!\")\n",
    "\t\t# Read Model\n",
    "\t\tprint(\"Loading the model...\")\n",
    "\t\tclassifier = pickle.load(file)\n",
    "\t\tfile.close()\n",
    "\n",
    "\t\tprint(\"Model load succesfully!\")\n",
    "\t\tclassifier.show_most_informative_features(5)\n",
    "  \n",
    "\t# Else (model unvailable)\n",
    "\texcept:\n",
    "\t\tprint(\"Model is not available!\")\n",
    "\t\tprint(\"Preparing for model training!\")\n",
    "\t\tclassifier = trainModel()\n",
    "  \n",
    "\treturn classifier\n",
    "\n",
    "def writeReview():\n",
    "    while True:\n",
    "        review = input(\"Input your review [>= 2 words]: \")\n",
    "        \n",
    "        words = review.split()\n",
    "        \n",
    "        if len(words) > 1:\n",
    "            print(\"Review added!\")\n",
    "            return review\n",
    "        else:\n",
    "            print(\"Your review must consisst of at least 2 words!\")\n",
    "     \n",
    "def analyzeReview(review, classifier):\n",
    "    if len(review) == 0:\n",
    "        print(\"Review is empty!\")\n",
    "        return\n",
    "    \n",
    "    # Tokenizing\n",
    "    words = word_tokenize(review.lower())\n",
    "    \n",
    "    # Frequency Distribution\n",
    "    words = FreqDist([word for word in words if word.isalpha() and word not in string.punctuation])\n",
    "    \n",
    "    # Tagging\n",
    "    tagged = pos_tag(words)\n",
    "    \n",
    "    print(\"Review Part of Speech Tag: \")\n",
    "    \n",
    "    for i, word in enumerate(tagged):\n",
    "        print(f\"{i+1}. {word[0]}, {word[1]}\")\n",
    "    \n",
    "    # Synonym and Antonym\n",
    "    for word in words:\n",
    "        print(\"==============\")\n",
    "        print(f\"Word: {word}\")\n",
    "        print(\"==============\")\n",
    "        \n",
    "        # Synsets\n",
    "        synsets = wordnet.synsets(word)\n",
    "        synonyms = []\n",
    "        antonyms = []\n",
    "        \n",
    "        for synset in synsets:\n",
    "            for lemma in synset.lemmas():\n",
    "                 synonyms.append(lemma.name())\n",
    "                 for antonym in lemma.antonyms():\n",
    "                     antonyms.append(antonym.name())\n",
    "        \n",
    "        print(\"Synonyms: \")\n",
    "        \n",
    "        if len(synonyms) == 0:\n",
    "            print(\"No synonym detected!\")\n",
    "        else:\n",
    "            for syn in synonyms[:5]:\n",
    "                print(f\"(+){syn}\")\n",
    "                \n",
    "        print(\"Antonyms: \")\n",
    "        \n",
    "        if len(antonyms) == 0:\n",
    "            print(\"No antonym detected!\")\n",
    "        else:\n",
    "            for antonym in antonyms[:5]:\n",
    "                print(f\"(-){antonym}\")\n",
    "                \n",
    "        print(\"===========================\")    \n",
    "\n",
    "\t# Predict the review\n",
    "    \n",
    "    # Preprocessing to remove punctuation and eng_stopwords and tokenize it\n",
    "    clean_review = [word for word in word_tokenize(review) if word not in string.punctuation and word not in eng_stopwords]\n",
    "    \n",
    "    # clean_review = [wnl.lemmatize(review) for review in clean_review]\n",
    "    # clean_review = [stemmer.stem(review) for review in clean_review]\n",
    "    \n",
    "    # Preprocessing to lemmatize and stemming the words\n",
    "    clean_review = [wnl.lemmatize(stemmer.stem(word)) for word in clean_review] \t\n",
    "    \n",
    "    result = classifier.classify(FreqDist(clean_review))\n",
    "    \n",
    "    print(f\"Your Review: {review}\")\n",
    "    print(f\"Review Category: {result}\")\n",
    "\t\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is available!\n",
      "Loading the model...\n",
      "Model load succesfully!\n",
      "Most Informative Features\n",
      "                 terribl = True           negati : positi =     12.6 : 1.0\n",
      "                 horribl = True           negati : positi =     10.0 : 1.0\n",
      "                  beauti = True           positi : negati =      9.3 : 1.0\n",
      "                 perfect = True           positi : negati =      8.9 : 1.0\n",
      "                      ok = True           negati : positi =      8.1 : 1.0\n",
      "Food Review Sentiment Analysis\n",
      "Your Review:  No Review\n",
      "1. Write your review\n",
      "2. Analyze your review\n",
      "3. Exit\n",
      ">> \n",
      "Review added!\n",
      "Food Review Sentiment Analysis\n",
      "Your Review:  This is just so bad lmao\n",
      "1. Write your review\n",
      "2. Analyze your review\n",
      "3. Exit\n",
      ">> \n",
      "Review Part of Speech Tag: \n",
      "1. this, DT\n",
      "2. is, VBZ\n",
      "3. just, RB\n",
      "4. so, RB\n",
      "5. bad, JJ\n",
      "6. lmao, NN\n",
      "==============\n",
      "Word: this\n",
      "==============\n",
      "Synonyms: \n",
      "No synonym detected!\n",
      "Antonyms: \n",
      "No antonym detected!\n",
      "===========================\n",
      "==============\n",
      "Word: is\n",
      "==============\n",
      "Synonyms: \n",
      "(+)be\n",
      "(+)be\n",
      "(+)be\n",
      "(+)exist\n",
      "(+)be\n",
      "Antonyms: \n",
      "(-)differ\n",
      "===========================\n",
      "==============\n",
      "Word: just\n",
      "==============\n",
      "Synonyms: \n",
      "(+)just\n",
      "(+)equitable\n",
      "(+)just\n",
      "(+)fair\n",
      "(+)just\n",
      "Antonyms: \n",
      "(-)unjust\n",
      "(-)inequitable\n",
      "(-)unfair\n",
      "===========================\n",
      "==============\n",
      "Word: so\n",
      "==============\n",
      "Synonyms: \n",
      "(+)sol\n",
      "(+)soh\n",
      "(+)so\n",
      "(+)so\n",
      "(+)so\n",
      "Antonyms: \n",
      "No antonym detected!\n",
      "===========================\n",
      "==============\n",
      "Word: bad\n",
      "==============\n",
      "Synonyms: \n",
      "(+)bad\n",
      "(+)badness\n",
      "(+)bad\n",
      "(+)bad\n",
      "(+)big\n",
      "Antonyms: \n",
      "(-)good\n",
      "(-)goodness\n",
      "(-)good\n",
      "(-)unregretful\n",
      "===========================\n",
      "==============\n",
      "Word: lmao\n",
      "==============\n",
      "Synonyms: \n",
      "No synonym detected!\n",
      "Antonyms: \n",
      "No antonym detected!\n",
      "===========================\n",
      "Your Review: This is just so bad lmao\n",
      "Review Category: negative\n",
      "Food Review Sentiment Analysis\n",
      "Your Review:  This is just so bad lmao\n",
      "1. Write your review\n",
      "2. Analyze your review\n",
      "3. Exit\n",
      ">> \n",
      "Thanks for using this application!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\t# Read Model\n",
    "\tclassifier = readModel()\n",
    "\t\n",
    "\t# Review\n",
    "\treview = \"\"\n",
    " \n",
    "\twhile True:\n",
    "\t\tprint(\"Food Review Sentiment Analysis\")\n",
    "\t\tprint(\"Your Review: \", \"No Review\" if len(review) == 0 else review)\n",
    "\t\tprint(\"1. Write your review\")\n",
    "\t\tprint(\"2. Analyze your review\")\n",
    "\t\tprint(\"3. Exit\")\n",
    "\t\tprint(\">> \")\n",
    "\t\tchoice = int(input(\">> \"))\n",
    "\t\tif (choice == 1):\n",
    "\t\t\treview = writeReview()\n",
    "\t\telif (choice == 2):\n",
    "\t\t\tanalyzeReview(review, classifier)\n",
    "\t\telif (choice == 3):\n",
    "\t\t\tprint(\"Thanks for using this application!\")\n",
    "\t\t\tbreak\n",
    "\t\telse:\n",
    "\t\t\tprint(\"Please only choose the available menu [1-3]!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
