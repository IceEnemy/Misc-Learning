{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised “minimum government maximum...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>with upcoming election india saga going import...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  category\n",
       "0  when modi promised “minimum government maximum...  negative\n",
       "1  what did just say vote for modi  welcome bjp t...  positive\n",
       "2  asking his supporters prefix chowkidar their n...  positive\n",
       "3  answer who among these the most powerful world...  positive\n",
       "4  with upcoming election india saga going import...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('dataset.csv')\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6894977168949772\n",
      "Most Informative Features\n",
      "                    poor = False          negati : positi =     13.4 : 1.0\n",
      "                  better = True           positi : negati =     11.3 : 1.0\n",
      "                    fail = False          negati : positi =     11.1 : 1.0\n",
      "                    find = False          negati : positi =      9.6 : 1.0\n",
      "                   idiot = False          negati : positi =      9.6 : 1.0\n",
      "              propaganda = False          negati : positi =      9.6 : 1.0\n",
      "                    hate = True           negati : positi =      9.2 : 1.0\n",
      "                   first = True           positi : negati =      8.9 : 1.0\n",
      "                  behind = False          negati : positi =      8.3 : 1.0\n",
      "                    best = True           positi : negati =      7.4 : 1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import FreqDist\n",
    "from random import shuffle\n",
    "import pickle\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    tokens = [lem.lemmatize(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "all_words = []\n",
    "for text in dataset[\"clean_text\"]:\n",
    "    # all_words.extend(preprocess_text(text))\n",
    "    for word in preprocess_text(text):\n",
    "        all_words.append(word)\n",
    "\n",
    "fd = FreqDist(all_words)\n",
    "\n",
    "featured_word = {word for word, count in fd.most_common(100)}\n",
    "\n",
    "feature_sets = []\n",
    "\n",
    "for text, category in zip(dataset[\"clean_text\"], dataset[\"category\"]):\n",
    "    feature = {}\n",
    "\n",
    "    processed_text = preprocess_text(text)\n",
    "    for word in processed_text:\n",
    "        feature[word] = word in featured_word\n",
    "\n",
    "    feature_sets.append((feature, category))\n",
    "\n",
    "# print(feature_sets)\n",
    "shuffle(feature_sets)\n",
    "\n",
    "split_index = int(len(feature_sets) * 0.8)\n",
    "train_set = feature_sets[:split_index]\n",
    "test_set = feature_sets[split_index:]\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# accuracy = nltk.classify.accuracy(classifier, test_set)\n",
    "accuracy = nltk.classify.util.accuracy(classifier, test_set)\n",
    "\n",
    "print(accuracy)\n",
    "\n",
    "classifier.show_most_informative_features(10)\n",
    "\n",
    "file = open('model.pickle', 'wb')\n",
    "pickle.dump(classifier, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def load_model():\n",
    "    if os.path.exists('./model.pickle'):\n",
    "        file = open('model.pickle', 'rb')\n",
    "        return pickle.load(file)\n",
    "    else:\n",
    "        return None \n",
    "    \n",
    "model = load_model()\n",
    "\n",
    "evaluated_sentence = \"beter, good, and hate\"\n",
    "\n",
    "fd_evaluate = FreqDist(word_tokenize(evaluated_sentence))\n",
    "category = model.classify(fd_evaluate)\n",
    "print(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: This\n",
      "---------------------------------------------\n",
      "Synonym\n",
      "No synonym\n",
      "Antonym\n",
      "No antonym\n",
      "---------------------------------------------\n",
      "Word: is\n",
      "---------------------------------------------\n",
      "Synonym\n",
      "(+)be\n",
      "(+)be\n",
      "(+)be\n",
      "(+)exist\n",
      "(+)be\n",
      "(+)be\n",
      "(+)equal\n",
      "(+)be\n",
      "(+)constitute\n",
      "(+)represent\n",
      "(+)make_up\n",
      "(+)comprise\n",
      "(+)be\n",
      "(+)be\n",
      "(+)follow\n",
      "(+)embody\n",
      "(+)be\n",
      "(+)personify\n",
      "(+)be\n",
      "(+)be\n",
      "(+)live\n",
      "(+)be\n",
      "(+)cost\n",
      "(+)be\n",
      "Antonym\n",
      "(-)differ\n",
      "---------------------------------------------\n",
      "Word: an\n",
      "---------------------------------------------\n",
      "Synonym\n",
      "(+)Associate_in_Nursing\n",
      "(+)AN\n",
      "Antonym\n",
      "No antonym\n",
      "---------------------------------------------\n",
      "Word: example\n",
      "---------------------------------------------\n",
      "Synonym\n",
      "(+)example\n",
      "(+)illustration\n",
      "(+)instance\n",
      "(+)representative\n",
      "(+)model\n",
      "(+)example\n",
      "(+)exemplar\n",
      "(+)example\n",
      "(+)model\n",
      "(+)good_example\n",
      "(+)example\n",
      "(+)deterrent_example\n",
      "(+)lesson\n",
      "(+)object_lesson\n",
      "(+)case\n",
      "(+)instance\n",
      "(+)example\n",
      "(+)exercise\n",
      "(+)example\n",
      "Antonym\n",
      "No antonym\n",
      "---------------------------------------------\n",
      "Word: text\n",
      "---------------------------------------------\n",
      "Synonym\n",
      "(+)text\n",
      "(+)textual_matter\n",
      "(+)text\n",
      "(+)textbook\n",
      "(+)text\n",
      "(+)text_edition\n",
      "(+)schoolbook\n",
      "(+)school_text\n",
      "(+)text\n",
      "Antonym\n",
      "(-)trade_edition\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# WordNet Synonym/Antonym Analysis\n",
    "from nltk.corpus import wordnet as wn\n",
    "import string\n",
    "\n",
    "def wordnet_analyze(text):\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    words = [word for word in words if word not in string.punctuation and word.isalpha()]\n",
    "\n",
    "    for word in words:\n",
    "        synsets = wn.synsets(word)\n",
    "\n",
    "        syn_list = []\n",
    "        antonym_list = []\n",
    "\n",
    "        for synset in synsets:\n",
    "            for lemma in synset.lemmas():\n",
    "                syn_list.append(lemma.name())\n",
    "                for antonym in lemma.antonyms():\n",
    "                    antonym_list.append(antonym.name())\n",
    "    \n",
    "\n",
    "        print(f\"Word: {word}\")\n",
    "        print(\"---------------------------------------------\")\n",
    "        print(\"Synonym\")\n",
    "        if len(syn_list) == 0:\n",
    "            print(\"No synonym\")\n",
    "        else:\n",
    "            for syn in syn_list:\n",
    "                print(f\"(+){syn}\")\n",
    "        \n",
    "        print(\"Antonym\")\n",
    "        if len(antonym_list) == 0:\n",
    "            print(\"No antonym\")\n",
    "        else:\n",
    "            for antonym in antonym_list:\n",
    "                print(f\"(-){antonym}\")\n",
    "        print(\"---------------------------------------------\")\n",
    "\n",
    "\n",
    "wordnet_analyze('This is an example text!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
